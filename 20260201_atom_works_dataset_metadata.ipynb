{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cba3a51",
   "metadata": {},
   "source": [
    "Atomworks datasets is like 3-stage pipeline:\n",
    "1. Dataset: index -> raw example\n",
    "2. Loader: raw example -> statandardized dict, typically include atom_array, example_id, extra_info\n",
    "3. Transform: standardized dict -> model-ready dict (cropping/atomize/encoding/features; keep annotation and featurization seperate)\n",
    "\n",
    "Protocol:\n",
    "1. Explore the raw data \n",
    "2. Define the rule for example_id (unique across dataset, tracable back to origin)\n",
    "3. Write loader (normalize metadata + parse structure; don't add model feature here)\n",
    "4. Write transform (split into annotation transforms then featurization transforms)\n",
    "5. Handle failtures (save_failed_examples_to_dir / fallback)\n",
    "6. Verify: random pick 10 sample for visualization + sanity stats like atom counts, chain IDs, missing coords\n",
    "7. Wire training IO (DataLoader + Sample + collate strategy for variable-length structures)\n",
    "8. Record idx to id to path to metadata (reversable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb16ca3",
   "metadata": {},
   "source": [
    "The metadata schema in Atomworks:\n",
    "A0. minimum you need to have:\n",
    "1. example_id: str -> need to be unique across all dataset\n",
    "2. path: str -> absolute str\n",
    "3. assembly_id: str -> choicable, always 1 as default\n",
    "4. dataset_name: str -> such as 'test_cifs'\n",
    "5. file_size: int -> check missing file\n",
    "6. file_ext: str (.cif/.cif.gz/.pdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f1729",
   "metadata": {},
   "source": [
    "# Unified metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10e407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional, Any, Dict, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class BuildMetaConfig:\n",
    "    dataset_name: str\n",
    "    base_dir: Path\n",
    "    assembly_id: str = \"1\"\n",
    "\n",
    "    # Example ID function: (path, rel_path, dataset_name) -> str\n",
    "    example_id_fn: Optional[Callable[[Path, Path, str], str]] = None\n",
    "\n",
    "    # Quick checks / enrichments\n",
    "    add_file_stats: bool = True\n",
    "    add_has_atom_site: bool = True\n",
    "\n",
    "    # Parse health-check (slower, but catches issues early)\n",
    "    do_parse_check: bool = True\n",
    "    parse_kwargs: Optional[Dict[str, Any]] = None  # passed to atomworks.io.parse\n",
    "    build_assembly: tuple[str, ...] = (\"1\",)\n",
    "\n",
    "    # Error string length to store in parquet\n",
    "    error_maxlen: int = 300\n",
    "\n",
    "\n",
    "def _default_example_id_fn(path: Path, rel: Path, dataset_name: str) -> str:\n",
    "    # Make a stable, mostly-readable ID\n",
    "    s = str(rel).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    s = s.replace(\".\", \"_\")\n",
    "    return f\"{dataset_name}__{s}\"\n",
    "\n",
    "\n",
    "def _quick_has_atom_site(path: Path, max_lines: int = 4000) -> bool:\n",
    "    # Heuristic: scan early lines for _atom_site.\n",
    "    # Works for text cif; for .gz you should decompress or skip this check.\n",
    "    try:\n",
    "        with path.open(\"rt\", errors=\"ignore\") as f:\n",
    "            for _ in range(max_lines):\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                if \"_atom_site.\" in line:\n",
    "                    return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def build_metadata_from_filedataset(\n",
    "    ds_raw: Iterable[Any],\n",
    "    out_parquet: str | Path,\n",
    "    cfg: BuildMetaConfig,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build AtomWorks-style metadata from a FileDataset-like iterable where ds_raw[i] yields PathLike.\n",
    "    \"\"\"\n",
    "    out_parquet = Path(out_parquet)\n",
    "\n",
    "    base_dir = Path(cfg.base_dir).resolve() \n",
    "    exid_fn = cfg.example_id_fn or _default_example_id_fn\n",
    "\n",
    "    # Optional parse import only if needed\n",
    "    parse = None\n",
    "    if cfg.do_parse_check:\n",
    "        from atomworks.io import parse as _parse\n",
    "        parse = _parse\n",
    "\n",
    "    # Default parse kwargs: avoid entity/chain_info pitfalls unless you really need them\n",
    "    parse_kwargs = {\n",
    "        \"add_missing_atoms\": False,\n",
    "        \"fix_formal_charges\": False,\n",
    "    }\n",
    "    if cfg.parse_kwargs:\n",
    "        parse_kwargs.update(cfg.parse_kwargs)\n",
    "\n",
    "    rows = []\n",
    "    for i, raw in enumerate(ds_raw):\n",
    "        p = Path(raw).resolve()\n",
    "        rel = p.relative_to(base_dir) if str(p).startswith(str(base_dir)) else Path(p.name)\n",
    "\n",
    "        row = {\n",
    "            \"dataset_name\": cfg.dataset_name,\n",
    "            \"example_id\": exid_fn(p, rel, cfg.dataset_name),\n",
    "            \"path\": str(p),\n",
    "            \"assembly_id\": str(cfg.assembly_id),\n",
    "            \"file_ext\": \"\".join(p.suffixes),\n",
    "        }\n",
    "\n",
    "        if cfg.add_file_stats:\n",
    "            try:\n",
    "                st = p.stat()\n",
    "                row[\"file_size\"] = int(st.st_size)\n",
    "                row[\"mtime\"] = float(st.st_mtime)\n",
    "            except Exception:\n",
    "                row[\"file_size\"] = None\n",
    "                row[\"mtime\"] = None\n",
    "\n",
    "        if cfg.add_has_atom_site:\n",
    "            # If gz, this heuristic won't work without decompressing. Keep False/None.\n",
    "            if row[\"file_ext\"].endswith(\".gz\"):\n",
    "                row[\"has_atom_site\"] = None\n",
    "            else:\n",
    "                row[\"has_atom_site\"] = _quick_has_atom_site(p)\n",
    "\n",
    "        # Parse check\n",
    "        row[\"parse_ok\"] = None\n",
    "        row[\"n_atoms\"] = None\n",
    "        row[\"error\"] = None\n",
    "        if cfg.do_parse_check and parse is not None:\n",
    "            try:\n",
    "                out = parse(\n",
    "                    filename=str(p),\n",
    "                    build_assembly=cfg.build_assembly,\n",
    "                    **parse_kwargs,\n",
    "                )\n",
    "                aa = out[\"assemblies\"][cfg.build_assembly[0]][0]\n",
    "                row[\"parse_ok\"] = True\n",
    "                row[\"n_atoms\"] = int(len(aa))\n",
    "            except Exception as e:\n",
    "                row[\"parse_ok\"] = False\n",
    "                msg = repr(e)\n",
    "                row[\"error\"] = msg[: cfg.error_maxlen]\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Attach dataset-level metadata in attrs (AtomWorks-style)\n",
    "    df.attrs[\"dataset_name\"] = cfg.dataset_name\n",
    "    df.attrs[\"base_dir\"] = str(base_dir)\n",
    "    df.attrs[\"schema_version\"] = \"atomworks_like_v1\"\n",
    "    df.attrs[\"notes\"] = (\n",
    "        \"Built from FileDataset; includes parse_ok/n_atoms/error if do_parse_check=True.\"\n",
    "    )\n",
    "\n",
    "    df.to_parquet(out_parquet, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c33e016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_ok\n",
      "True    3\n",
      "Name: count, dtype: int64\n",
      "dataset attributes metadata:\n",
      "{'dataset_name': 'test_cifs', 'base_dir': '/home/eva/20251207_python_playground/20260109_playground_biotite_atomworks/example_cifs', 'schema_version': 'atomworks_like_v1', 'notes': 'Built from FileDataset; includes parse_ok/n_atoms/error if do_parse_check=True.'}\n",
      "dataset metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>example_id</th>\n",
       "      <th>path</th>\n",
       "      <th>assembly_id</th>\n",
       "      <th>file_ext</th>\n",
       "      <th>file_size</th>\n",
       "      <th>mtime</th>\n",
       "      <th>has_atom_site</th>\n",
       "      <th>parse_ok</th>\n",
       "      <th>n_atoms</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_cifs</td>\n",
       "      <td>test_cifs__7ee8_cif</td>\n",
       "      <td>/home/eva/20251207_python_playground/20260109_...</td>\n",
       "      <td>1</td>\n",
       "      <td>.cif</td>\n",
       "      <td>628536</td>\n",
       "      <td>1.769950e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2628</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_cifs</td>\n",
       "      <td>test_cifs__7h66_cif</td>\n",
       "      <td>/home/eva/20251207_python_playground/20260109_...</td>\n",
       "      <td>1</td>\n",
       "      <td>.cif</td>\n",
       "      <td>303738</td>\n",
       "      <td>1.769950e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1764</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_cifs</td>\n",
       "      <td>test_cifs__7h68_cif</td>\n",
       "      <td>/home/eva/20251207_python_playground/20260109_...</td>\n",
       "      <td>1</td>\n",
       "      <td>.cif</td>\n",
       "      <td>853667</td>\n",
       "      <td>1.769950e+09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3575</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name           example_id  \\\n",
       "0    test_cifs  test_cifs__7ee8_cif   \n",
       "1    test_cifs  test_cifs__7h66_cif   \n",
       "2    test_cifs  test_cifs__7h68_cif   \n",
       "\n",
       "                                                path assembly_id file_ext  \\\n",
       "0  /home/eva/20251207_python_playground/20260109_...           1     .cif   \n",
       "1  /home/eva/20251207_python_playground/20260109_...           1     .cif   \n",
       "2  /home/eva/20251207_python_playground/20260109_...           1     .cif   \n",
       "\n",
       "   file_size         mtime  has_atom_site  parse_ok  n_atoms error  \n",
       "0     628536  1.769950e+09           True      True     2628  None  \n",
       "1     303738  1.769950e+09           True      True     1764  None  \n",
       "2     853667  1.769950e+09           True      True     3575  None  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from atomworks.ml.datasets import FileDataset\n",
    "from pathlib import Path\n",
    "\n",
    "ds_raw = FileDataset.from_directory(\n",
    "    directory=\"/home/eva/20251207_python_playground/20260109_playground_biotite_atomworks/example_cifs\",\n",
    "    name=\"test_cifs\",\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "cfg = BuildMetaConfig(\n",
    "    dataset_name=\"test_cifs\",\n",
    "    base_dir=Path(\"/home/eva/20251207_python_playground/20260109_playground_biotite_atomworks/example_cifs\"),\n",
    "    do_parse_check=True,\n",
    "    build_assembly=(\"1\",),\n",
    "    # you could add parse_kwargs to cover default values\n",
    ")\n",
    "\n",
    "df = build_metadata_from_filedataset(\n",
    "    ds_raw=ds_raw,\n",
    "    out_parquet=\"test_cifs_metadata.parquet\",\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "print(df[\"parse_ok\"].value_counts(dropna=False))\n",
    "\n",
    "# print the df.attrs for dataset-level metadata\n",
    "print(\"dataset attributes metadata:\")\n",
    "print(df.attrs)\n",
    "\n",
    "print(\"dataset metadata:\")\n",
    "df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8a2f9",
   "metadata": {},
   "source": [
    "# To write metadata manually into FileDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72cf5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>path</th>\n",
       "      <th>assembly_id</th>\n",
       "      <th>parse_ok</th>\n",
       "      <th>n_atoms</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7ee8</td>\n",
       "      <td>/home/eva/20251207_python_playground/20260109_...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2628</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7h66</td>\n",
       "      <td>/home/eva/20251207_python_playground/20260109_...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1764</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7h68</td>\n",
       "      <td>/home/eva/20251207_python_playground/20260109_...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3575</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  example_id                                               path assembly_id  \\\n",
       "0       7ee8  /home/eva/20251207_python_playground/20260109_...           1   \n",
       "1       7h66  /home/eva/20251207_python_playground/20260109_...           1   \n",
       "2       7h68  /home/eva/20251207_python_playground/20260109_...           1   \n",
       "\n",
       "   parse_ok  n_atoms error  \n",
       "0      True     2628  None  \n",
       "1      True     1764  None  \n",
       "2      True     3575  None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from atomworks.io import parse\n",
    "from atomworks.ml.datasets import FileDataset\n",
    "\n",
    "# Generate a dataset\n",
    "ds_raw = FileDataset.from_directory(\n",
    "    directory=\"/home/eva/20251207_python_playground/20260109_playground_biotite_atomworks/example_cifs\",\n",
    "    name=\"test_cifs\",\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "# Parse each file and collect metadata\n",
    "rows = []\n",
    "for i in range(len(ds_raw)):\n",
    "    p = Path(ds_raw[i]).resolve()\n",
    "    row = {\n",
    "        \"example_id\": p.stem,\n",
    "        \"path\": str(p),\n",
    "        \"assembly_id\": \"1\",\n",
    "        \"parse_ok\": False,\n",
    "        \"n_atoms\": None,\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        out = parse(\n",
    "            filename=str(p),\n",
    "            build_assembly=(\"1\",),\n",
    "            add_missing_atoms=False,\n",
    "            fix_formal_charges=False,\n",
    "            add_id_and_entity_annotations=False,  # Get around with bad annotations\n",
    "        )\n",
    "        aa = out[\"assemblies\"][\"1\"][0]\n",
    "        row[\"parse_ok\"] = True\n",
    "        row[\"n_atoms\"] = len(aa)\n",
    "    except Exception as e:\n",
    "        row[\"error\"] = repr(e)[:300]  \n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# Save metadata to Parquet\n",
    "df = pd.DataFrame(rows)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f38481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_ok\n",
      "True    3\n",
      "Name: count, dtype: int64\n",
      "Bad examples (first 5):\n",
      "Empty DataFrame\n",
      "Columns: [example_id, path, error]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df.attrs[\"dataset_name\"] = \"test_cifs\"\n",
    "df.attrs[\"source\"] = \"local_example_cifs\"\n",
    "df.attrs[\"created_by\"] = \"eva\"\n",
    "df.attrs[\"notes\"] = \"Filtered parse_ok only\"\n",
    "\n",
    "df.to_parquet(\"test_cifs_metadata.parquet\", index=False)\n",
    "\n",
    "print(df[\"parse_ok\"].value_counts())\n",
    "print(\"Bad examples (first 5):\")\n",
    "print(df.loc[~df[\"parse_ok\"], [\"example_id\",\"path\",\"error\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11a2aa",
   "metadata": {},
   "source": [
    "# Read parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eacd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['example_id', 'path', 'assembly_id', 'parse_ok', 'n_atoms', 'error']\n",
      "  example_id                                               path assembly_id  \\\n",
      "0       7ee8  /home/eva/20251207_python_playground/20260109_...           1   \n",
      "1       7h66  /home/eva/20251207_python_playground/20260109_...           1   \n",
      "\n",
      "   parse_ok  n_atoms error  \n",
      "0      True     2628  None  \n",
      "1      True     1764  None  \n"
     ]
    }
   ],
   "source": [
    "# Load and filter parsed metadata - this will not include the attrs!!\n",
    "df = pd.read_parquet(\"test_cifs_metadata.parquet\")\n",
    "\n",
    "# print columns and first 2 rows\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head(2))\n",
    "df_ok = df[df[\"parse_ok\"]].reset_index(drop=True)\n",
    "df_ok.to_parquet(\"test_cifs_metadata_ok.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b44b59",
   "metadata": {},
   "source": [
    "# You could restart from here to rebuild dataset from parquet only (the path to cif file not changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d65e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata'])\n",
      "example_id: 7ee8\n",
      "atom_array atoms: 5261\n",
      "dataset metadata:\n",
      "{'id': '7ee8', 'method': 'X-RAY_DIFFRACTION', 'deposition_date': '2021-03-17', 'release_date': '2025-03-26', 'resolution': 1.22, 'extra_metadata': None, 'crystallization_details': {'pH': [7.5, 7.5]}}\n",
      "extra info:\n",
      "{'PANDAS_ATTRS': '{\"dataset_name\": \"test_cifs\", \"source\": \"local_example_cifs\", \"created_by\": \"eva\", \"notes\": \"Filtered parse_ok only\"}', 'parse_ok': True, 'n_atoms': 2628, 'error': None}\n",
      "atom_array:\n",
      "['chain_id', 'res_id', 'ins_code', 'res_name', 'hetero', 'atom_name', 'element', 'stereo', 'alt_atom_id', 'is_aromatic', 'is_polymer', 'occupancy', 'chain_type', 'uses_alt_atom_id', 'charge', 'is_backbone_atom', 'b_factor', 'pn_unit_id', 'molecule_id', 'chain_entity', 'pn_unit_entity', 'molecule_entity', 'atomic_number', 'transformation_id', 'chain_iid', 'pn_unit_iid', 'molecule_iid']\n",
      "chain_id                  type=<U4 example=['A' 'A' 'A' 'A' 'A']\n",
      "res_id                    type=int64 example=[1 1 1 1 1]\n",
      "ins_code                  type=<U1 example=['' '' '' '' '']\n",
      "res_name                  type=<U5 example=['PRO' 'PRO' 'PRO' 'PRO' 'PRO']\n",
      "hetero                    type=bool example=[False False False False False]\n",
      "atom_name                 type=<U6 example=['N' 'CA' 'C' 'O' 'CB']\n",
      "element                   type=<U2 example=['N' 'C' 'C' 'O' 'C']\n",
      "stereo                    type=<U1 example=['N' 'S' 'N' 'N' 'N']\n",
      "alt_atom_id               type=<U4 example=['N' 'CA' 'C' 'O' 'CB']\n",
      "is_aromatic               type=bool example=[False False False False False]\n",
      "is_polymer                type=bool example=[ True  True  True  True  True]\n",
      "occupancy                 type=float64 example=[1. 1. 1. 1. 1.]\n",
      "chain_type                type=int64 example=[6 6 6 6 6]\n",
      "uses_alt_atom_id          type=bool example=[False False False False False]\n",
      "charge                    type=int8 example=[0 0 0 0 0]\n",
      "is_backbone_atom          type=bool example=[ True  True  True  True False]\n",
      "b_factor                  type=float64 example=[14.73 14.88 14.84 14.13 15.69]\n",
      "pn_unit_id                type=<U1 example=['A' 'A' 'A' 'A' 'A']\n",
      "molecule_id               type=int16 example=[0 0 0 0 0]\n",
      "chain_entity              type=int64 example=[0 0 0 0 0]\n",
      "pn_unit_entity            type=int64 example=[0 0 0 0 0]\n",
      "molecule_entity           type=int64 example=[0 0 0 0 0]\n",
      "atomic_number             type=int8 example=[7 6 6 8 6]\n",
      "transformation_id         type=<U1 example=['1' '1' '1' '1' '1']\n",
      "chain_iid                 type=<U3 example=['A_1' 'A_1' 'A_1' 'A_1' 'A_1']\n",
      "pn_unit_iid               type=<U3 example=['A_1' 'A_1' 'A_1' 'A_1' 'A_1']\n",
      "molecule_iid              type=int16 example=[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from atomworks.ml.datasets import PandasDataset\n",
    "from atomworks.ml.datasets.loaders import create_base_loader\n",
    "\n",
    "out_path = \"test_cifs_metadata_ok.parquet\"\n",
    "\n",
    "loader = create_base_loader(\n",
    "    example_id_colname=\"example_id\",\n",
    "    path_colname=\"path\",\n",
    "    assembly_id_colname=\"assembly_id\",\n",
    ")\n",
    "\n",
    "ds = PandasDataset(\n",
    "    data=out_path,\n",
    "    name=\"test_cifs_from_parquet_ok\",\n",
    "    loader=loader,\n",
    ")\n",
    "\n",
    "ex = ds[0]\n",
    "\n",
    "# Inspect the first example\n",
    "print(ex.keys())\n",
    "print(\"example_id:\", ex[\"example_id\"])\n",
    "print(\"atom_array atoms:\", len(ex[\"atom_array\"]))\n",
    "\n",
    "print(\"dataset metadata:\")\n",
    "print(ex['metadata'])\n",
    "\n",
    "print(\"extra info:\")\n",
    "print(ex['extra_info'])\n",
    "\n",
    "print(\"atom_array:\")\n",
    "print(ex['atom_array'].get_annotation_categories())\n",
    "\n",
    "aa = ex[\"atom_array\"]\n",
    "\n",
    "for name in aa.get_annotation_categories():\n",
    "    values = getattr(aa, name)\n",
    "    print(\n",
    "        f\"{name:25s}\",\n",
    "        f\"type={values.dtype if hasattr(values, 'dtype') else type(values)}\",\n",
    "        f\"example={values[:5]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e741138",
   "metadata": {},
   "source": [
    "Aim:\n",
    "\n",
    "Input: cif folder path\n",
    "\n",
    "output 1：chains_metadata.parquet（each row is one chain）\n",
    "\n",
    "output 2：interfaces_metadata.parquet（each row is a chain pair which satisfy the contact condition）\n",
    "\n",
    "The parquet can be directly load with PandasDataset + create_loader_with_query_pn_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58aa6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CIFs: 3\n",
      "chains rows: 5\n",
      "interfaces rows: 3\n",
      "bad files: 0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biotite.structure as struc\n",
    "\n",
    "from atomworks.io import parse\n",
    "\n",
    "# ---------- Parameter ----------\n",
    "CIF_DIR = Path(\"/home/eva/20251207_python_playground/20260109_playground_biotite_atomworks/example_cifs\").resolve()\n",
    "ASSEMBLY_ID = \"1\"\n",
    "\n",
    "# Parameters for interface definition\n",
    "CONTACT_CUTOFF_A = 5.0      # atom-atom distance cutoff (Å)\n",
    "MIN_CONTACTS = 20           # minimum number of contacts to define an interface\n",
    "CELL_SIZE = 6.0             # cell list cell size (Å)\n",
    "\n",
    "\n",
    "def safe_parse(path: Path):\n",
    "    \"\"\"\n",
    "    - add_missing_atoms=False / fix_formal_charges=False avoid modifying the structure\n",
    "    - build_assembly: only build the specified assembly to save time\n",
    "    \"\"\"\n",
    "    return parse(\n",
    "        filename=str(path),\n",
    "        build_assembly=(ASSEMBLY_ID,),\n",
    "        add_missing_atoms=False,\n",
    "        fix_formal_charges=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_polymer_chain_units(atom_array: struc.AtomArray) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extract polymer chain units from an AtomArray.\n",
    "      pn_unit_iid, chain_id, atom_indices, n_atoms\n",
    "    \"\"\"\n",
    "    # only care about polymer atoms\n",
    "    poly = atom_array[atom_array.is_polymer]\n",
    "\n",
    "    # pn_unit_iid is what we consider as \"polymer chain\"\n",
    "    units = []\n",
    "    for iid in np.unique(poly.pn_unit_iid):\n",
    "        mask = (poly.pn_unit_iid == iid)\n",
    "        idx = np.where(mask)[0] # local indices in polymer atom array\n",
    "        chain_ids = np.unique(poly.chain_id[mask])\n",
    "        chain_id = chain_ids[0] if len(chain_ids) else None\n",
    "\n",
    "        units.append({\n",
    "            \"pn_unit_iid\": str(iid),\n",
    "            \"chain_id\": str(chain_id) if chain_id is not None else None,\n",
    "            \"poly_local_indices\": idx,\n",
    "            \"n_atoms\": int(mask.sum()),\n",
    "        })\n",
    "    return units\n",
    "\n",
    "\n",
    "def count_chain_contacts(atom_array: struc.AtomArray, iid1: str, iid2: str,\n",
    "                         cutoff: float = CONTACT_CUTOFF_A) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Calculate number of contacts and minimum distance between two polymer chains.\n",
    "    Contacts are defined as atom pairs within the specified cutoff distance.\n",
    "    \"\"\"\n",
    "    aa = atom_array[atom_array.is_polymer]\n",
    "    m1 = (aa.pn_unit_iid == iid1)\n",
    "    m2 = (aa.pn_unit_iid == iid2)\n",
    "    if not (np.any(m1) and np.any(m2)):\n",
    "        return 0, float(\"inf\")\n",
    "\n",
    "    coord1 = aa.coord[m1]\n",
    "    coord2 = aa.coord[m2]\n",
    "\n",
    "    # remove NaN coords\n",
    "    v1 = ~np.isnan(coord1).any(axis=1)\n",
    "    v2 = ~np.isnan(coord2).any(axis=1)\n",
    "    coord1 = coord1[v1]\n",
    "    coord2 = coord2[v2]\n",
    "    if len(coord1) == 0 or len(coord2) == 0:\n",
    "        return 0, float(\"inf\")\n",
    "\n",
    "    cl = struc.CellList(coord2, cell_size=CELL_SIZE)\n",
    "    near = cl.get_atoms(coord1, cutoff, as_mask=True)\n",
    "    has_neighbor = np.any(near, axis=1)\n",
    "    n_contacts = int(np.sum(has_neighbor))\n",
    "\n",
    "    # calculate min distance\n",
    "    min_dist = float(\"inf\")\n",
    "    if n_contacts > 0:\n",
    "        # only check those with neighbors\n",
    "        for i, rowmask in enumerate(near):\n",
    "            if not np.any(rowmask):\n",
    "                continue\n",
    "            d = np.linalg.norm(coord2[rowmask] - coord1[i], axis=1)\n",
    "            md = float(d.min())\n",
    "            if md < min_dist:\n",
    "                min_dist = md\n",
    "\n",
    "    return n_contacts, min_dist\n",
    "\n",
    "\n",
    "def make_file_id(path: Path, base_dir: Path) -> str:\n",
    "    # Make a unique file ID from path relative to base_dir\n",
    "    rel = path.relative_to(base_dir)\n",
    "    return str(rel).replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "\n",
    "# ---------- Main ----------\n",
    "chain_rows = []\n",
    "iface_rows = []\n",
    "bad_rows = []\n",
    "\n",
    "cif_paths = sorted([p for p in CIF_DIR.rglob(\"*\") if p.is_file() and \"\".join(p.suffixes).endswith(\".cif\")])\n",
    "print(\"Found CIFs:\", len(cif_paths))\n",
    "\n",
    "for p in cif_paths:\n",
    "    file_id = make_file_id(p, CIF_DIR)\n",
    "\n",
    "    try:\n",
    "        out = safe_parse(p)\n",
    "        aa = out[\"assemblies\"][ASSEMBLY_ID][0]\n",
    "    except Exception as e:\n",
    "        bad_rows.append({\n",
    "            \"file_id\": file_id,\n",
    "            \"path\": str(p),\n",
    "            \"error\": repr(e)[:300],\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # ---- chains-level ----\n",
    "    units = get_polymer_chain_units(aa)\n",
    "\n",
    "    for u in units:\n",
    "        chain_rows.append({\n",
    "            \"example_id\": f\"{file_id}__{u['pn_unit_iid']}\",\n",
    "            \"path\": str(p),\n",
    "            \"assembly_id\": ASSEMBLY_ID,\n",
    "            \"pn_unit_iid\": u[\"pn_unit_iid\"],\n",
    "            \"chain_id\": u[\"chain_id\"],\n",
    "            \"file_id\": file_id,\n",
    "            \"n_atoms_chain\": u[\"n_atoms\"],\n",
    "        })\n",
    "\n",
    "    # ---- interfaces-level ----\n",
    "    # 只在“聚合物链之间”找界面（你也可以扩展到 ligand）\n",
    "    iids = [u[\"pn_unit_iid\"] for u in units]\n",
    "    for iid1, iid2 in itertools.combinations(sorted(iids), 2):\n",
    "        n_contacts, min_dist = count_chain_contacts(aa, iid1, iid2, cutoff=CONTACT_CUTOFF_A)\n",
    "        if n_contacts >= MIN_CONTACTS:\n",
    "            iface_rows.append({\n",
    "                \"example_id\": f\"{file_id}__{iid1}__{iid2}\",\n",
    "                \"path\": str(p),\n",
    "                \"assembly_id\": ASSEMBLY_ID,\n",
    "                \"pn_unit_1_iid\": iid1,\n",
    "                \"pn_unit_2_iid\": iid2,\n",
    "                \"file_id\": file_id,\n",
    "                \"n_contacts\": int(n_contacts),\n",
    "                \"min_dist\": float(min_dist),\n",
    "                \"interface_type\": \"polymer-polymer\",\n",
    "            })\n",
    "\n",
    "# ---------- Write parquet ----------\n",
    "df_chains = pd.DataFrame(chain_rows)\n",
    "df_ifaces = pd.DataFrame(iface_rows)\n",
    "df_bad = pd.DataFrame(bad_rows)\n",
    "\n",
    "df_chains.to_parquet(\"chains_metadata.parquet\", index=False)\n",
    "df_ifaces.to_parquet(\"interfaces_metadata.parquet\", index=False)\n",
    "df_bad.to_csv(\"bad_files.csv\", index=False)\n",
    "\n",
    "print(\"chains rows:\", len(df_chains))\n",
    "print(\"interfaces rows:\", len(df_ifaces))\n",
    "print(\"bad files:\", len(df_bad))\n",
    "print(df_bad.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0389cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5 examples\n",
      "7ee8_cif__A_1 ['A_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'chain_id': 'A', 'file_id': '7ee8_cif', 'n_atoms_chain': 867}\n",
      "    A       1  PRO N      N       -30.570  -19.082   12.020\n",
      "    A       1  PRO CA     C       -29.427  -18.387   11.424\n",
      "7ee8_cif__B_1 ['B_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'chain_id': 'B', 'file_id': '7ee8_cif', 'n_atoms_chain': 863}\n",
      "    A       1  PRO N      N       -30.570  -19.082   12.020\n",
      "    A       1  PRO CA     C       -29.427  -18.387   11.424\n",
      "7ee8_cif__C_1 ['C_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'chain_id': 'C', 'file_id': '7ee8_cif', 'n_atoms_chain': 865}\n",
      "    A       1  PRO N      N       -30.570  -19.082   12.020\n",
      "    A       1  PRO CA     C       -29.427  -18.387   11.424\n",
      "7h66_cif__A_1 ['A_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'chain_id': 'A', 'file_id': '7h66_cif', 'n_atoms_chain': 1733}\n",
      "    A       1  ILE N      N       -29.820   20.926   -4.596\n",
      "    A       1  ILE CA     C       -28.920   21.363   -5.690\n",
      "7h68_cif__A_1 ['A_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'chain_id': 'A', 'file_id': '7h68_cif', 'n_atoms_chain': 3539}\n",
      "    A       1  ILE N      N        57.670    7.014    4.429\n",
      "    A       1  ILE CA     C        58.094    7.945    5.513\n"
     ]
    }
   ],
   "source": [
    "from atomworks.ml.datasets import PandasDataset\n",
    "from atomworks.ml.datasets.loaders import create_loader_with_query_pn_units\n",
    "\n",
    "loader_chain = create_loader_with_query_pn_units(\n",
    "    pn_unit_iid_colnames=\"pn_unit_iid\",\n",
    "    parser_args={\"add_id_and_entity_annotations\": False},\n",
    ")\n",
    "\n",
    "ds_chain = PandasDataset(\n",
    "    data=\"chains_metadata.parquet\",\n",
    "    name=\"chains_ds\",\n",
    "    loader=loader_chain,\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(ds_chain)} examples\")\n",
    "\n",
    "# print all examples\n",
    "for i in range(len(ds_chain)):\n",
    "    ex = ds_chain[i]\n",
    "    print(ex[\"example_id\"], ex[\"query_pn_unit_iids\"])\n",
    "    print(ex.keys())\n",
    "    print(ex['extra_info'])\n",
    "    print(ex['atom_array'][0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aec4aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7ee8_cif__A_1__B_1 ['A_1', 'B_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'file_id': '7ee8_cif', 'n_contacts': 148, 'min_dist': 2.676819324493408, 'interface_type': 'polymer-polymer'}\n",
      "    A       1  PRO N      N       -30.570  -19.082   12.020\n",
      "    A       1  PRO CA     C       -29.427  -18.387   11.424\n",
      "7ee8_cif__A_1__C_1 ['A_1', 'C_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'file_id': '7ee8_cif', 'n_contacts': 148, 'min_dist': 2.662930727005005, 'interface_type': 'polymer-polymer'}\n",
      "    A       1  PRO N      N       -30.570  -19.082   12.020\n",
      "    A       1  PRO CA     C       -29.427  -18.387   11.424\n",
      "7ee8_cif__B_1__C_1 ['B_1', 'C_1']\n",
      "dict_keys(['example_id', 'path', 'assembly_id', 'extra_info', 'atom_array', 'atom_array_stack', 'chain_info', 'ligand_info', 'metadata', 'query_pn_unit_iids'])\n",
      "{'file_id': '7ee8_cif', 'n_contacts': 151, 'min_dist': 2.6665048599243164, 'interface_type': 'polymer-polymer'}\n",
      "    A       1  PRO N      N       -30.570  -19.082   12.020\n",
      "    A       1  PRO CA     C       -29.427  -18.387   11.424\n"
     ]
    }
   ],
   "source": [
    "loader_iface = create_loader_with_query_pn_units(\n",
    "    pn_unit_iid_colnames=[\"pn_unit_1_iid\", \"pn_unit_2_iid\"],\n",
    "    parser_args={\"add_id_and_entity_annotations\": False},\n",
    ")\n",
    "\n",
    "ds_iface = PandasDataset(\n",
    "    data=\"interfaces_metadata.parquet\",\n",
    "    name=\"ifaces_ds\",\n",
    "    loader=loader_iface,\n",
    ")\n",
    "\n",
    "for i in range(len(ds_iface)):\n",
    "    ex = ds_iface[i]\n",
    "    print(ex[\"example_id\"], ex[\"query_pn_unit_iids\"])\n",
    "    print(ex.keys())\n",
    "    print(ex['extra_info'])\n",
    "    print(ex['atom_array'][0:2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atomworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
